{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fb00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d56114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Error type: constant ===\n",
      "        code  backend_size  logical_error_rate  deviation  abs_deviation\n",
      "35     bacon           300               0.530   0.272000       0.272000\n",
      "161    color           400               0.492   0.270333       0.270333\n",
      "81     gross           400               0.001  -0.015000       0.015000\n",
      "114       hh           300               0.147  -0.224533       0.224533\n",
      "104   steane           500               0.443   0.139667       0.139667\n",
      "143  surface           400               0.452   0.310733       0.310733\n",
      "\n",
      "Average absolute deviation per code:\n",
      "       code  avg_abs_deviation\n",
      "0     bacon           0.181600\n",
      "2     color           0.172089\n",
      "4     gross           0.010500\n",
      "6        hh           0.142089\n",
      "8    steane           0.085956\n",
      "10  surface           0.184089\n",
      "\n",
      "=== Error type: modsi1000 ===\n",
      "        code  backend_size  logical_error_rate  deviation  abs_deviation\n",
      "62     bacon           300               0.014  -0.302467       0.302467\n",
      "163    color           400               0.523   0.324933       0.324933\n",
      "5      gross           350               0.029   0.008400       0.008400\n",
      "120       hh           350               0.519   0.263533       0.263533\n",
      "99    steane           400               0.254   0.121867       0.121867\n",
      "158  surface           500               0.494   0.339400       0.339400\n",
      "\n",
      "Average absolute deviation per code:\n",
      "       code  avg_abs_deviation\n",
      "1     bacon           0.198578\n",
      "3     color           0.203956\n",
      "5     gross           0.005000\n",
      "7        hh           0.165156\n",
      "9    steane           0.075511\n",
      "11  surface           0.204800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('../experiment_results/Size_full/results.csv')\n",
    "\n",
    "# Exclude error_probability == 0.008\n",
    "df = df[~((df[\"code\"] == \"gross\") & (df[\"error_probability\"] == 0.008))]\n",
    "\n",
    "# Compute average logical error rate per code and error_type\n",
    "avg_df = df.groupby([\"code\", \"error_type\"])[\"logical_error_rate\"].mean().reset_index()\n",
    "avg_df.rename(columns={\"logical_error_rate\": \"avg_logical_error_rate\"}, inplace=True)\n",
    "\n",
    "# Merge back to compute absolute deviation\n",
    "df = df.merge(avg_df, on=[\"code\", \"error_type\"])\n",
    "df[\"deviation\"] = df[\"logical_error_rate\"] - df[\"avg_logical_error_rate\"]\n",
    "df[\"abs_deviation\"] = df[\"deviation\"].abs()  # absolute deviation\n",
    "\n",
    "# Find backend size with maximum deviation for each code and error_type\n",
    "max_dev_idx = df.groupby([\"code\", \"error_type\"])[\"abs_deviation\"].idxmax()\n",
    "max_dev_df = df.loc[max_dev_idx, [\"code\", \"error_type\", \"backend_size\", \"logical_error_rate\", \"deviation\", \"abs_deviation\"]]\n",
    "\n",
    "# Compute average absolute deviation per code and error_type\n",
    "avg_dev_df = df.groupby([\"code\", \"error_type\"])[\"abs_deviation\"].mean().reset_index()\n",
    "avg_dev_df.rename(columns={\"abs_deviation\": \"avg_abs_deviation\"}, inplace=True)\n",
    "\n",
    "# Print separate tables per error type\n",
    "error_types = max_dev_df[\"error_type\"].unique()\n",
    "for e_type in error_types:\n",
    "    table = max_dev_df[max_dev_df[\"error_type\"] == e_type].sort_values(\"code\")\n",
    "    print(f\"\\n=== Error type: {e_type} ===\")\n",
    "    print(table[[\"code\", \"backend_size\", \"logical_error_rate\", \"deviation\", \"abs_deviation\"]])\n",
    "\n",
    "    # Also print average deviation per code\n",
    "    avg_dev_table = avg_dev_df[avg_dev_df[\"error_type\"] == e_type].sort_values(\"code\")\n",
    "    print(\"\\nAverage absolute deviation per code:\")\n",
    "    print(avg_dev_table[[\"code\", \"avg_abs_deviation\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3091bd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Error type: constant ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>backend_size</th>\n",
       "      <th>decoder</th>\n",
       "      <th>logical_error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>surface</td>\n",
       "      <td>300</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>surface</td>\n",
       "      <td>350</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>surface</td>\n",
       "      <td>400</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>surface</td>\n",
       "      <td>450</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>surface</td>\n",
       "      <td>500</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>bacon</td>\n",
       "      <td>350</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>bacon</td>\n",
       "      <td>400</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>gross</td>\n",
       "      <td>400</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>bacon</td>\n",
       "      <td>300</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>bacon</td>\n",
       "      <td>500</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>gross</td>\n",
       "      <td>350</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code  backend_size decoder  logical_error_rate\n",
       "108  surface           300   bposd               0.000\n",
       "109  surface           350   bposd               0.000\n",
       "130  surface           400   bposd               0.000\n",
       "132  surface           450   bposd               0.000\n",
       "152  surface           500   bposd               0.000\n",
       "61     bacon           350   bposd               0.001\n",
       "63     bacon           400   bposd               0.001\n",
       "81     gross           400   bposd               0.001\n",
       "60     bacon           300   bposd               0.002\n",
       "65     bacon           500   bposd               0.002\n",
       "80     gross           350   bposd               0.002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Error type: modsi1000 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>backend_size</th>\n",
       "      <th>decoder</th>\n",
       "      <th>logical_error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>surface</td>\n",
       "      <td>300</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>surface</td>\n",
       "      <td>350</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>surface</td>\n",
       "      <td>400</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>surface</td>\n",
       "      <td>450</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>surface</td>\n",
       "      <td>500</td>\n",
       "      <td>bposd</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code  backend_size decoder  logical_error_rate\n",
       "110  surface           300   bposd               0.000\n",
       "111  surface           350   bposd               0.000\n",
       "134  surface           400   bposd               0.000\n",
       "131  surface           450   bposd               0.001\n",
       "153  surface           500   bposd               0.001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Cell 2: List codes with good logical error rate at p=0.002 ---\n",
    "\n",
    "# Filter for error_probability = 0.002\n",
    "filtered_df = df[df[\"error_probability\"] == 0.002]\n",
    "\n",
    "# Group by code and error_type and find which have logical_error_rate <= 0.002\n",
    "good_df = (\n",
    "    filtered_df[filtered_df[\"logical_error_rate\"] <= 0.002]\n",
    "    .sort_values([\"error_type\", \"logical_error_rate\"])\n",
    ")\n",
    "\n",
    "# Print results per error type\n",
    "error_types = good_df[\"error_type\"].unique()\n",
    "\n",
    "for e_type in error_types:\n",
    "    subset = good_df[good_df[\"error_type\"] == e_type]\n",
    "    print(f\"\\n=== Error type: {e_type} ===\")\n",
    "    if subset.empty:\n",
    "        print(\"No codes found with logical_error_rate ≤ 0.002 at p=0.002.\")\n",
    "        continue\n",
    "    display(\n",
    "        subset[[\"code\", \"backend_size\", \"decoder\", \"logical_error_rate\"]]\n",
    "        if \"decoder\" in subset.columns\n",
    "        else subset[[\"code\", \"backend_size\", \"logical_error_rate\"]]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1abf1745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      code  cube_to_full_drop_pct\n",
      "0    bacon              86.538462\n",
      "1    color              95.873786\n",
      "2    gross              98.398398\n",
      "3       hh              71.818182\n",
      "4   steane              50.000000\n",
      "5  surface             100.000000\n",
      "\n",
      "Average cube→full drop: 83.77%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../experiment_results/Connectivity_small/results.csv')\n",
    "\n",
    "# Define custom backend order\n",
    "backend_order = ['custom_grid', 'custom_cube', 'custom_full']\n",
    "df['backend'] = pd.Categorical(df['backend'], categories=backend_order, ordered=True)\n",
    "\n",
    "# Sort by code and backend\n",
    "df_sorted = df.sort_values(by=['code', 'backend']).reset_index(drop=True)\n",
    "\n",
    "# Compute drop from cube to full for each code\n",
    "cube_to_full_drops = []\n",
    "for code, group in df_sorted.groupby('code'):\n",
    "    group = group.sort_values('backend').reset_index(drop=True)\n",
    "    # Only compute if both cube and full exist\n",
    "    if {'custom_cube', 'custom_full'}.issubset(set(group['backend'])):\n",
    "        cube_rate = group.loc[group['backend'] == 'custom_cube', 'logical_error_rate'].values[0]\n",
    "        full_rate = group.loc[group['backend'] == 'custom_full', 'logical_error_rate'].values[0]\n",
    "        drop_pct = (cube_rate - full_rate) / cube_rate * 100\n",
    "        cube_to_full_drops.append({'code': code, 'cube_to_full_drop_pct': drop_pct})\n",
    "\n",
    "df_cube_to_full = pd.DataFrame(cube_to_full_drops)\n",
    "\n",
    "print(df_cube_to_full)\n",
    "avg_drop = df_cube_to_full['cube_to_full_drop_pct'].mean()\n",
    "print(f\"\\nAverage cube→full drop: {avg_drop:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5e04942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      code  grid_to_full_drop_pct\n",
      "0    bacon              85.858586\n",
      "1    color              96.523517\n",
      "2    gross              98.400000\n",
      "3       hh              72.197309\n",
      "4   steane              17.000000\n",
      "5  surface             100.000000\n",
      "\n",
      "Average grid→full drop: 78.33%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../experiment_results/Connectivity_small/results.csv')\n",
    "\n",
    "# Define custom backend order\n",
    "backend_order = ['custom_grid', 'custom_cube', 'custom_full']\n",
    "df['backend'] = pd.Categorical(df['backend'], categories=backend_order, ordered=True)\n",
    "\n",
    "# Sort by code and backend\n",
    "df_sorted = df.sort_values(by=['code', 'backend']).reset_index(drop=True)\n",
    "\n",
    "# Compute drop from grid to full for each code\n",
    "grid_to_full_drops = []\n",
    "for code, group in df_sorted.groupby('code'):\n",
    "    group = group.sort_values('backend').reset_index(drop=True)\n",
    "    # Only compute if both grid and full exist\n",
    "    if {'custom_grid', 'custom_full'}.issubset(set(group['backend'])):\n",
    "        grid_rate = group.loc[group['backend'] == 'custom_grid', 'logical_error_rate'].values[0]\n",
    "        full_rate = group.loc[group['backend'] == 'custom_full', 'logical_error_rate'].values[0]\n",
    "        drop_pct = (grid_rate - full_rate) / grid_rate * 100\n",
    "        grid_to_full_drops.append({'code': code, 'grid_to_full_drop_pct': drop_pct})\n",
    "\n",
    "df_grid_to_full = pd.DataFrame(grid_to_full_drops)\n",
    "\n",
    "print(df_grid_to_full)\n",
    "avg_drop = df_grid_to_full['grid_to_full_drop_pct'].mean()\n",
    "print(f\"\\nAverage grid→full drop: {avg_drop:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981afcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      code  cube_to_full_drop_pct\n",
      "0    bacon              -5.050505\n",
      "1    color              15.746421\n",
      "2    gross               0.100000\n",
      "3       hh               1.345291\n",
      "4   steane             -66.000000\n",
      "5  surface              28.776978\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../experiment_results/Connectivity_small/results.csv')\n",
    "\n",
    "# Define custom backend order\n",
    "backend_order = ['custom_grid', 'custom_cube', 'custom_full']\n",
    "df['backend'] = pd.Categorical(df['backend'], categories=backend_order, ordered=True)\n",
    "\n",
    "# Sort by code and backend\n",
    "df_sorted = df.sort_values(by=['code', 'backend']).reset_index(drop=True)\n",
    "\n",
    "# Compute drop from cube to full for each code\n",
    "grid_to_cube_drops = []\n",
    "for code, group in df_sorted.groupby('code'):\n",
    "    group = group.sort_values('backend').reset_index(drop=True)\n",
    "    # Only compute if both cube and full exist\n",
    "    if {'custom_cube', 'custom_full'}.issubset(set(group['backend'])):\n",
    "        cube_rate = group.loc[group['backend'] == 'custom_cube', 'logical_error_rate'].values[0]\n",
    "        grid_rate = group.loc[group['backend'] == 'custom_grid', 'logical_error_rate'].values[0]\n",
    "        drop_pct = (grid_rate - cube_rate) / grid_rate * 100\n",
    "        grid_to_cube_drops.append({'code': code, 'cube_to_full_drop_pct': drop_pct})\n",
    "\n",
    "df_grid_to_cube = pd.DataFrame(grid_to_cube_drops)\n",
    "\n",
    "print(df_grid_to_cube)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61870a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend  variance_low  variance_mid  variance_high  mid_vs_low  high_vs_mid  \\\n",
      "code                                                                          \n",
      "bacon           0.487         0.493          0.491       0.006       -0.002   \n",
      "color           0.498         0.486          0.508      -0.012        0.022   \n",
      "gross           1.000         1.000          0.999       0.000       -0.001   \n",
      "hh              0.497         0.504          0.513       0.007        0.009   \n",
      "steane          0.023         0.023          0.016       0.000       -0.007   \n",
      "surface         0.512         0.498          0.518      -0.014        0.020   \n",
      "\n",
      "backend  high_vs_low  \n",
      "code                  \n",
      "bacon          0.004  \n",
      "color          0.010  \n",
      "gross         -0.001  \n",
      "hh             0.016  \n",
      "steane        -0.007  \n",
      "surface        0.006  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26426/3272860985.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  error_summary = df_sorted.groupby(['code', 'backend'])['logical_error_rate'].mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and sort data\n",
    "df = pd.read_csv('../experiment_results/Variance/results.csv')\n",
    "backend_order = ['variance_low', 'variance_mid', 'variance_high']\n",
    "df['backend'] = pd.Categorical(df['backend'], categories=backend_order, ordered=True)\n",
    "df_sorted = df.sort_values(by=['code', 'backend']).reset_index(drop=True)\n",
    "\n",
    "# Group by code and backend to get mean logical error rate\n",
    "error_summary = df_sorted.groupby(['code', 'backend'])['logical_error_rate'].mean().reset_index()\n",
    "\n",
    "# Pivot so backends are columns\n",
    "error_pivot = error_summary.pivot(index='code', columns='backend', values='logical_error_rate')\n",
    "\n",
    "# Calculate differences between backends\n",
    "error_pivot['mid_vs_low'] = error_pivot['variance_mid'] - error_pivot['variance_low']\n",
    "error_pivot['high_vs_mid'] = error_pivot['variance_high'] - error_pivot['variance_mid']\n",
    "error_pivot['high_vs_low'] = error_pivot['variance_high'] - error_pivot['variance_low']\n",
    "\n",
    "print(error_pivot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f7fbc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code with largest variance effect (percent): steane\n",
      "Largest percentage difference in logical error rate: 30.43%\n",
      "backend  mid_vs_low_%  high_vs_mid_%  high_vs_low_%  max_diff_%\n",
      "code                                                           \n",
      "steane       0.000000     -30.434783     -30.434783   30.434783\n",
      "color       -2.409639       4.526749       2.008032    4.526749\n",
      "surface     -2.734375       4.016064       1.171875    4.016064\n",
      "hh           1.408451       1.785714       3.219316    3.219316\n",
      "bacon        1.232033      -0.405680       0.821355    1.232033\n",
      "gross        0.000000      -0.100000      -0.100000    0.100000\n"
     ]
    }
   ],
   "source": [
    "# Assuming error_pivot already exists with variance_low/mid/high columns\n",
    "\n",
    "# Compute percentage differences\n",
    "error_pivot['mid_vs_low_%'] = ((error_pivot['variance_mid'] - error_pivot['variance_low']) \n",
    "                               / error_pivot['variance_low'] * 100)\n",
    "\n",
    "error_pivot['high_vs_mid_%'] = ((error_pivot['variance_high'] - error_pivot['variance_mid']) \n",
    "                                / error_pivot['variance_mid'] * 100)\n",
    "\n",
    "error_pivot['high_vs_low_%'] = ((error_pivot['variance_high'] - error_pivot['variance_low']) \n",
    "                                / error_pivot['variance_low'] * 100)\n",
    "\n",
    "# Find maximum percentage difference per code\n",
    "error_pivot['max_diff_%'] = error_pivot[['mid_vs_low_%', 'high_vs_mid_%', 'high_vs_low_%']].abs().max(axis=1)\n",
    "\n",
    "# Code with largest percentage difference\n",
    "biggest_diff_code = error_pivot['max_diff_%'].idxmax()\n",
    "biggest_diff_value = error_pivot.loc[biggest_diff_code, 'max_diff_%']\n",
    "\n",
    "print(\"Code with largest variance effect (percent):\", biggest_diff_code)\n",
    "print(\"Largest percentage difference in logical error rate: {:.2f}%\".format(biggest_diff_value))\n",
    "\n",
    "# Optional: sort by percentage difference\n",
    "sorted_diffs = error_pivot.sort_values(by='max_diff_%', ascending=False)\n",
    "print(sorted_diffs[['mid_vs_low_%', 'high_vs_mid_%', 'high_vs_low_%', 'max_diff_%']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c54252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-code averages:\n",
      "backend  real_apollo  real_nsinfleqtion  real_willow\n",
      "code                                                \n",
      "bacon          0.518              0.482        0.496\n",
      "color          0.172              0.497        0.522\n",
      "gross          0.024              0.999        0.999\n",
      "hh             0.162              0.507        0.486\n",
      "steane         0.437              0.049        0.073\n",
      "surface        0.000              0.480        0.490\n",
      "\n",
      "Average per backend:\n",
      "backend\n",
      "real_apollo          0.218833\n",
      "real_nsinfleqtion    0.502333\n",
      "real_willow          0.511000\n",
      "dtype: float64\n",
      "\n",
      "Combined averages:\n",
      "{'willow+nsinfleqtion': 0.5066666666666667, 'apollo': 0.21883333333333332}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and sort data\n",
    "df = pd.read_csv('../experiment_results/Topology/results.csv')\n",
    "df = df[df[\"backend\"] != \"real_infleqtion\"]\n",
    "df_sorted = df.sort_values(by=['code', 'backend']).reset_index(drop=True)\n",
    "\n",
    "# Group by code and backend to get mean logical error rate\n",
    "error_summary = df_sorted.groupby(['code', 'backend'])['logical_error_rate'].mean().reset_index()\n",
    "\n",
    "# Pivot so backends are columns\n",
    "error_pivot = error_summary.pivot(index='code', columns='backend', values='logical_error_rate')\n",
    "print(\"Per-code averages:\")\n",
    "print(error_pivot)\n",
    "\n",
    "# Average across codes for each backend\n",
    "backend_means = error_pivot.mean(axis=0)\n",
    "print(\"\\nAverage per backend:\")\n",
    "print(backend_means)\n",
    "\n",
    "# Combined averages\n",
    "combined = {\n",
    "    \"willow+nsinfleqtion\": error_pivot[['real_willow', 'real_nsinfleqtion']].mean(axis=1).mean(),\n",
    "    \"apollo\": error_pivot[['real_apollo']].mean(axis=1).mean()\n",
    "}\n",
    "print(\"\\nCombined averages:\")\n",
    "print(combined)\n",
    "\n",
    "# General average per code across all available backends\n",
    "per_code_avg = error_pivot.mean(axis=1)\n",
    "print(\"\\nGeneral average per code:\")\n",
    "print(per_code_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5354a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Normal Results =====\n",
      "dataset  DQC 1 QPU Size  DQC Full Size  abs_improvement  pct_improvement\n",
      "code                                                                    \n",
      "surface           0.033          0.481            0.448        93.139293\n",
      "color             0.191          0.502            0.311        61.952191\n",
      "hh                0.192          0.496            0.304        61.290323\n",
      "steane            0.063          0.079            0.016        20.253165\n",
      "bacon             0.420          0.513            0.093        18.128655\n",
      "\n",
      "Average absolute improvement: 0.2344\n",
      "Average percentage improvement: 50.95272529654406\n",
      "\n",
      "===== Lower Results =====\n",
      "dataset  DQC 1 QPU Size  DQC Full Size  abs_improvement  pct_improvement\n",
      "code                                                                    \n",
      "surface           0.009          0.301            0.292        97.009967\n",
      "hh                0.119          0.439            0.320        72.892938\n",
      "color             0.183          0.503            0.320        63.618290\n",
      "bacon             0.339          0.502            0.163        32.470120\n",
      "steane            0.054          0.075            0.021        28.000000\n",
      "\n",
      "Average absolute improvement: 0.22319999999999998\n",
      "Average percentage improvement: 58.798263010870684\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = \"../experiment_results\"\n",
    "\n",
    "datasets_normal = [\n",
    "    (\"DQC\", \"DQC Full Size\"),\n",
    "    (\"DQC_1_QPU\", \"DQC 1 QPU Size\")\n",
    "]\n",
    "\n",
    "datasets_lower = [\n",
    "    (\"DQC_LOWER\", \"DQC Full Size\"),\n",
    "    (\"DQC_1_QPU_LOWER\", \"DQC 1 QPU Size\")\n",
    "]\n",
    "\n",
    "def compute_summary(datasets, label):\n",
    "    dfs = []\n",
    "    for folder, ds_label in datasets:\n",
    "        tech_path = os.path.join(path, folder, \"results.csv\")\n",
    "        df_tmp = pd.read_csv(tech_path)\n",
    "        df_tmp[\"dataset\"] = ds_label\n",
    "        dfs.append(df_tmp)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Exclude gross\n",
    "    df = df[df[\"code\"].str.lower() != \"gross\"]\n",
    "\n",
    "    # Group by code & dataset to compute mean error\n",
    "    summary = df.groupby([\"code\", \"dataset\"])[\"logical_error_rate\"].mean().reset_index()\n",
    "\n",
    "    # Pivot so datasets become columns\n",
    "    pivot = summary.pivot(index=\"code\", columns=\"dataset\", values=\"logical_error_rate\")\n",
    "\n",
    "    # Compute improvements\n",
    "    pivot[\"abs_improvement\"] = pivot[\"DQC Full Size\"] - pivot[\"DQC 1 QPU Size\"]\n",
    "    pivot[\"pct_improvement\"] = (pivot[\"abs_improvement\"] / pivot[\"DQC Full Size\"]) * 100\n",
    "    pivot[\"group\"] = label  # tag Normal vs Lower\n",
    "\n",
    "    # Compute overall averages\n",
    "    avg_abs = pivot[\"abs_improvement\"].mean()\n",
    "    avg_pct = pivot[\"pct_improvement\"].mean()\n",
    "\n",
    "    print(f\"\\n===== {label} Results =====\")\n",
    "    print(pivot.sort_values(\"pct_improvement\", ascending=False))\n",
    "    print(\"\\nAverage absolute improvement:\", avg_abs)\n",
    "    print(\"Average percentage improvement:\", avg_pct)\n",
    "\n",
    "    return pivot\n",
    "\n",
    "# Run for both sets\n",
    "pivot_normal = compute_summary(datasets_normal, \"Normal\")\n",
    "pivot_lower = compute_summary(datasets_lower, \"Lower\")\n",
    "\n",
    "# Combine both for global averages\n",
    "combined = pd.concat([pivot_normal, pivot_lower], ignore_index=True)\n",
    "\n",
    "overall_avg_abs = combined[\"abs_improvement\"].mean()\n",
    "overall_avg_pct = combined[\"pct_improvement\"].mean()\n",
    "\n",
    "print(\"\\n===== Overall Averages Across Normal + Lower =====\")\n",
    "print(\"Overall average absolute improvement:\", overall_avg_abs)\n",
    "print(\"Overall average percentage improvement:\", overall_avg_pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Lower Results =====\n",
      "dataset  DQC 1 QPU Size  DQC Full Size  abs_improvement  pct_improvement\n",
      "code                                                                    \n",
      "bacon             0.052          0.522            0.470        90.038314\n",
      "hh                0.003          0.002           -0.001       -50.000000\n",
      "color             0.006          0.003           -0.003      -100.000000\n",
      "steane            0.048          0.021           -0.027      -128.571429\n",
      "surface           0.001          0.000           -0.001             -inf\n",
      "\n",
      "Average absolute improvement: 0.0876\n",
      "Average percentage improvement: -inf\n",
      "\n",
      "===== Normal Results =====\n",
      "dataset  DQC 1 QPU Size  DQC Full Size  abs_improvement  pct_improvement\n",
      "code                                                                    \n",
      "bacon             0.155          0.524            0.369        70.419847\n",
      "color             0.005          0.014            0.009        64.285714\n",
      "steane            0.061          0.058           -0.003        -5.172414\n",
      "hh                0.014          0.012           -0.002       -16.666667\n",
      "surface           0.001          0.000           -0.001             -inf\n",
      "\n",
      "Average absolute improvement: 0.0744\n",
      "Average percentage improvement: -inf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the single CSV file\n",
    "path = \"../experiment_results/DQC_Nighthawk/results.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Exclude gross and routing_method = basic\n",
    "df = df[(df[\"code\"].str.lower() != \"gross\") & (df[\"routing_method\"].str.lower() != \"basic\")]\n",
    "\n",
    "# Map backend to dataset labels\n",
    "dataset_map = {\n",
    "    \"real_nighthawk\": \"DQC Full Size\",\n",
    "    \"real_nighthawk_1_qpu\": \"DQC 1 QPU Size\"\n",
    "}\n",
    "df[\"dataset\"] = df[\"backend\"].map(dataset_map)\n",
    "\n",
    "# Map error_probability to groups\n",
    "group_map = {1: \"Normal\", 10: \"Lower\"}\n",
    "df[\"group\"] = df[\"error_probability\"].map(group_map)\n",
    "\n",
    "# Group by code, dataset, and group to compute mean error\n",
    "summary = df.groupby([\"code\", \"dataset\", \"group\"])[\"logical_error_rate\"].mean().reset_index()\n",
    "\n",
    "# Iterate over groups (Normal, Lower)\n",
    "for grp, subdf in summary.groupby(\"group\"):\n",
    "    pivot = subdf.pivot(index=\"code\", columns=\"dataset\", values=\"logical_error_rate\")\n",
    "\n",
    "    # Compute improvements\n",
    "    pivot[\"abs_improvement\"] = pivot[\"DQC Full Size\"] - pivot[\"DQC 1 QPU Size\"]\n",
    "    if pivot[\"DQC Full Size\"] != 0:\n",
    "        pivot[\"pct_improvement\"] = (pivot[\"abs_improvement\"] / pivot[\"DQC Full Size\"]) * 100\n",
    "    else:\n",
    "        pivot[\"pct_improvement\"]\n",
    "    print(f\"\\n===== {grp} Results =====\")\n",
    "    print(pivot.sort_values(\"pct_improvement\", ascending=False))\n",
    "\n",
    "    # Group averages\n",
    "    avg_abs = pivot[\"abs_improvement\"].mean()\n",
    "    avg_pct = pivot[\"pct_improvement\"].mean()\n",
    "    print(\"\\nAverage absolute improvement:\", avg_abs)\n",
    "    print(\"Average percentage improvement:\", avg_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c238c671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average per backend:\n",
      "              backend  logical_error_rate\n",
      "0        real_apollo            0.005667\n",
      "1    real_infleqtion            0.244667\n",
      "2      real_nsapollo            0.000833\n",
      "3  real_nsinfleqtion            0.496667\n",
      "4        real_willow            0.567500\n",
      "\n",
      "Average per code:\n",
      "       code  logical_error_rate\n",
      "0    bacon              0.2940\n",
      "1    color              0.2028\n",
      "2    gross              0.5776\n",
      "3       hh              0.2142\n",
      "4   steane              0.0900\n",
      "5  surface              0.1998\n",
      "NS: 0.24875\n",
      "S: 0.125167\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------\n",
    "# Load all backend datasets\n",
    "# -----------------------\n",
    "\n",
    "# Backend 1 (combined from two files)\n",
    "df1 = pd.read_csv(\"../experiment_results/Willow/results.csv\")\n",
    "df2 = pd.read_csv(\"../experiment_results/Infleqtion/results.csv\")\n",
    "df3 = pd.read_csv(\"../experiment_results/Apollo/results.csv\")\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "backend_avg = df.groupby(\"backend\")[\"logical_error_rate\"].mean().reset_index()\n",
    "\n",
    "# -----------------------\n",
    "# Average per code\n",
    "# -----------------------\n",
    "code_avg = df.groupby(\"code\")[\"logical_error_rate\"].mean().reset_index()\n",
    "\n",
    "print(\"Average per backend:\\n\", backend_avg)\n",
    "print(\"\\nAverage per code:\\n\", code_avg)\n",
    "\n",
    "print(f\"NS: {(0.000833 + 0.496667)/2}\")\n",
    "print(f\"S: {(0.005667 + 0.244667)/2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cca177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average per backend (mean + var):\n",
      "        backend  gate_overhead_mean  gate_overhead_var\n",
      "0  custom_grid        25080.044815      385852.408816\n",
      "\n",
      "Average per code:\n",
      "       code  swap_overhead_mean\n",
      "0    bacon         1197.249778\n",
      "1    color        18329.354333\n",
      "2    gross        82506.326667\n",
      "3       hh        18650.467556\n",
      "4   steane         9158.226000\n",
      "5  surface        20638.644556\n",
      "\n",
      "Average per routing method:\n",
      "   routing_method  swap_overhead_mean\n",
      "0          basic        34703.670889\n",
      "1          sabre        15231.403722\n",
      "2     stochastic        25305.059833\n",
      "\n",
      "Average per routing+layout combination (mean + var):\n",
      "   routing_method layout_method  gate_overhead_mean  gate_overhead_var\n",
      "0          basic         dense        37831.333333       0.000000e+00\n",
      "1          basic         sabre        30779.512667       1.076677e+06\n",
      "2          basic       trivial        35500.166667       0.000000e+00\n",
      "3          sabre         dense        18312.346833       1.415587e+05\n",
      "4          sabre         sabre        10216.480167       6.821185e+04\n",
      "5          sabre       trivial        17165.384167       1.148935e+05\n",
      "6     stochastic         dense        27982.242667       9.075714e+05\n",
      "7     stochastic         sabre        21331.024833       2.739104e+05\n",
      "8     stochastic       trivial        26601.912000       8.898491e+05\n",
      "\n",
      "Best routing+layout combination per code:\n",
      "       code best_routing best_layout  best_value\n",
      "0    bacon        basic       sabre       0.000\n",
      "1    color        sabre       sabre    5307.796\n",
      "2    gross        sabre       sabre   44415.493\n",
      "3       hh        sabre       sabre    4516.717\n",
      "4   steane        sabre       sabre    3770.592\n",
      "5  surface        sabre       sabre    3288.283\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------\n",
    "# Load backend dataset\n",
    "# -----------------------\n",
    "df = pd.read_csv(\"../experiment_results/Routing_grid/results.csv\")\n",
    "\n",
    "# -----------------------\n",
    "# Mean + variance per backend\n",
    "# -----------------------\n",
    "backend_stats = (\n",
    "    df.groupby(\"backend\", as_index=False)\n",
    "      .agg(\n",
    "          gate_overhead_mean=(\"swap_overhead_mean\", \"mean\"),\n",
    "          gate_overhead_var=(\"swap_overhead_var\", \"mean\")  # assumes column exists\n",
    "      )\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Average per code\n",
    "# -----------------------\n",
    "code_avg = df.groupby(\"code\")[\"swap_overhead_mean\"].mean().reset_index()\n",
    "\n",
    "# -----------------------\n",
    "# Average per routing method\n",
    "# -----------------------\n",
    "routing_avg = df.groupby(\"routing_method\")[\"swap_overhead_mean\"].mean().reset_index()\n",
    "\n",
    "# -----------------------\n",
    "# Average per routing + layout method combination\n",
    "# -----------------------\n",
    "combo_avg = (\n",
    "    df.groupby([\"routing_method\", \"layout_method\"], as_index=False)\n",
    "      .agg(\n",
    "          gate_overhead_mean=(\"swap_overhead_mean\", \"mean\"),\n",
    "          gate_overhead_var=(\"swap_overhead_var\", \"mean\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# Pivot table form (just for means)\n",
    "combo_pivot = (\n",
    "    df.pivot_table(\n",
    "        index=\"routing_method\",\n",
    "        columns=\"layout_method\",\n",
    "        values=\"swap_overhead_mean\",\n",
    "        aggfunc=\"mean\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Best routing+layout per code\n",
    "# -----------------------\n",
    "# Compute mean overhead per code & combination\n",
    "code_combo = (\n",
    "    df.groupby([\"code\", \"routing_method\", \"layout_method\"], as_index=False)\n",
    "      .agg(best_gate_overhead=(\"swap_overhead_mean\", \"mean\"))\n",
    ")\n",
    "\n",
    "# For each code, find the row with the minimum swap_overhead_mean\n",
    "best_per_code = (\n",
    "    code_combo.loc[code_combo.groupby(\"code\")[\"best_gate_overhead\"].idxmin()]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "best_per_code.rename(columns={\n",
    "    \"routing_method\": \"best_routing\",\n",
    "    \"layout_method\": \"best_layout\",\n",
    "    \"best_gate_overhead\": \"best_value\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Make sure pandas prints everything\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# -----------------------\n",
    "# Print all results\n",
    "# -----------------------\n",
    "print(\"Average per backend (mean + var):\\n\", backend_stats)\n",
    "print(\"\\nAverage per code:\\n\", code_avg)\n",
    "print(\"\\nAverage per routing method:\\n\", routing_avg)\n",
    "print(\"\\nAverage per routing+layout combination (mean + var):\\n\", combo_avg)\n",
    "print(\"\\nBest routing+layout combination per code:\\n\", best_per_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaaa019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average per backend (mean + var + %):\n",
      "            backend  gate_overhead_mean  gate_overhead_var  gate_overhead_pct\n",
      "0  custom_heavyhex          12305.3332       245729.00557         317.508386\n",
      "\n",
      "Average per code (raw + %):\n",
      "       code      raw_mean    norm_pct\n",
      "0    bacon   2396.845889  220.298335\n",
      "1    color  12581.235333  238.280972\n",
      "2       hh  18245.859111  259.174135\n",
      "3   steane  13580.108222  565.602175\n",
      "4  surface  14722.617444  304.186311\n",
      "\n",
      "Average per routing method (raw + %):\n",
      "   routing_method      raw_mean    norm_pct\n",
      "0          basic  16679.556667  417.378770\n",
      "1          sabre   8341.445000  222.472302\n",
      "2     stochastic  11894.997933  312.674085\n",
      "\n",
      "Average per routing+layout combination (raw + var + %):\n",
      "   routing_method layout_method    raw_mean       raw_var    norm_pct\n",
      "0          basic         dense  17251.4000  0.000000e+00  423.991566\n",
      "1          basic         sabre  12508.6700  1.068834e+06  317.081487\n",
      "2          basic       trivial  20278.6000  0.000000e+00  511.063258\n",
      "3          sabre         dense   9351.6254  1.002189e+05  246.000216\n",
      "4          sabre         sabre   4789.2756  6.969034e+04  131.847656\n",
      "5          sabre       trivial  10883.4340  7.720734e+04  289.569033\n",
      "6     stochastic         dense  12644.5284  3.795094e+05  327.597625\n",
      "7     stochastic         sabre   8840.5944  2.228919e+05  232.722132\n",
      "8     stochastic       trivial  14199.8710  2.932088e+05  377.702498\n",
      "\n",
      "Best routing+layout combination per code (% criterion):\n",
      "       code best_routing best_layout  best_raw_value  best_norm_pct\n",
      "0    bacon        sabre       sabre         892.139      81.998070\n",
      "1    color        sabre       sabre        5257.670      99.577083\n",
      "2       hh        sabre       sabre        6201.596      88.090852\n",
      "3   steane        sabre       sabre        7147.208     297.676302\n",
      "4  surface        sabre       sabre        4447.765      91.895971\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Load routing dataset\n",
    "# -----------------------\n",
    "#df = pd.read_csv(\"../experiment_results/Routing_grid/results.csv\")\n",
    "df = pd.read_csv(\"../experiment_results/Routing_hh/results.csv\")  # change this path as needed\n",
    "\n",
    "# Merge into one dataset\n",
    "#df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Load tq_gates_total from Translation results\n",
    "# -----------------------\n",
    "df_gates = pd.read_csv(\"../experiment_results/Translation/results.csv\")\n",
    "\n",
    "# Keep only relevant columns: code + original_2q_gates\n",
    "df_gates = df_gates[[\"code\", \"original_2q_gates\"]].drop_duplicates()\n",
    "\n",
    "# -----------------------\n",
    "# Merge on code\n",
    "# -----------------------\n",
    "df = df.merge(df_gates, on=\"code\", how=\"left\")\n",
    "\n",
    "# -----------------------\n",
    "# Add normalized percentage value\n",
    "# -----------------------\n",
    "df[\"swap_overhead_pct\"] = (df[\"swap_overhead_mean\"] / df[\"original_2q_gates\"]) * 100\n",
    "\n",
    "# -----------------------\n",
    "# Mean + variance per backend\n",
    "# -----------------------\n",
    "backend_stats = (\n",
    "    df.groupby(\"backend\", as_index=False)\n",
    "      .agg(\n",
    "          gate_overhead_mean=(\"swap_overhead_mean\", \"mean\"),\n",
    "          gate_overhead_var=(\"swap_overhead_var\", \"mean\"),\n",
    "          gate_overhead_pct=(\"swap_overhead_pct\", \"mean\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Average per code (percentage)\n",
    "# -----------------------\n",
    "code_avg = (\n",
    "    df.groupby(\"code\", as_index=False)\n",
    "      .agg(\n",
    "          raw_mean=(\"swap_overhead_mean\", \"mean\"),\n",
    "          norm_pct=(\"swap_overhead_pct\", \"mean\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Average per routing method (percentage)\n",
    "# -----------------------\n",
    "routing_avg = (\n",
    "    df.groupby(\"routing_method\", as_index=False)\n",
    "      .agg(\n",
    "          raw_mean=(\"swap_overhead_mean\", \"mean\"),\n",
    "          norm_pct=(\"swap_overhead_pct\", \"mean\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Average per routing + layout method combination (percentage)\n",
    "# -----------------------\n",
    "combo_avg = (\n",
    "    df.groupby([\"routing_method\", \"layout_method\"], as_index=False)\n",
    "      .agg(\n",
    "          raw_mean=(\"swap_overhead_mean\", \"mean\"),\n",
    "          raw_var=(\"swap_overhead_var\", \"mean\"),\n",
    "          norm_pct=(\"swap_overhead_pct\", \"mean\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# Pivot table form (just normalized percentages)\n",
    "combo_pivot = (\n",
    "    df.pivot_table(\n",
    "        index=\"routing_method\",\n",
    "        columns=\"layout_method\",\n",
    "        values=\"swap_overhead_pct\",\n",
    "        aggfunc=\"mean\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Best routing+layout per code (percentage criterion)\n",
    "# -----------------------\n",
    "code_combo = (\n",
    "    df.groupby([\"code\", \"routing_method\", \"layout_method\"], as_index=False)\n",
    "      .agg(\n",
    "          raw_mean=(\"swap_overhead_mean\", \"mean\"),\n",
    "          norm_pct=(\"swap_overhead_pct\", \"mean\")\n",
    "      )\n",
    ")\n",
    "\n",
    "best_per_code = (\n",
    "    code_combo.loc[code_combo.groupby(\"code\")[\"norm_pct\"].idxmin()]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "best_per_code.rename(columns={\n",
    "    \"routing_method\": \"best_routing\",\n",
    "    \"layout_method\": \"best_layout\",\n",
    "    \"norm_pct\": \"best_norm_pct\",\n",
    "    \"raw_mean\": \"best_raw_value\"\n",
    "}, inplace=True)\n",
    "\n",
    "# -----------------------\n",
    "# Print results\n",
    "# -----------------------\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "print(\"Average per backend (mean + var + %):\\n\", backend_stats)\n",
    "print(\"\\nAverage per code (raw + %):\\n\", code_avg)\n",
    "print(\"\\nAverage per routing method (raw + %):\\n\", routing_avg)\n",
    "print(\"\\nAverage per routing+layout combination (raw + var + %):\\n\", combo_avg)\n",
    "print(\"\\nBest routing+layout combination per code (% criterion):\\n\", best_per_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5f151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "General average across all codes and backends: 12305.3332\n",
      "\n",
      "Average swap_overhead_mean for sabre x sabre combination: 4789.2756\n",
      "\n",
      "Average swap_overhead_mean for sabre x sabre, code=hh, backend=custom_heavyhex: 6201.596\n",
      "Average per backend (mean + var):\n",
      "            backend  gate_overhead_mean  gate_overhead_var\n",
      "0  custom_heavyhex          12305.3332       245729.00557\n",
      "\n",
      "Average per code (across all backends):\n",
      "       code  swap_overhead_mean\n",
      "0    bacon         2396.845889\n",
      "1    color        12581.235333\n",
      "2       hh        18245.859111\n",
      "3   steane        13580.108222\n",
      "4  surface        14722.617444\n",
      "\n",
      "Average per routing method (across all backends):\n",
      "   routing_method  swap_overhead_mean\n",
      "0          basic        16679.556667\n",
      "1          sabre         8341.445000\n",
      "2     stochastic        11894.997933\n",
      "\n",
      "Average per routing+layout combination (mean + var):\n",
      "   routing_method layout_method  gate_overhead_mean  gate_overhead_var\n",
      "0          basic         dense          17251.4000       0.000000e+00\n",
      "1          basic         sabre          12508.6700       1.068834e+06\n",
      "2          basic       trivial          20278.6000       0.000000e+00\n",
      "3          sabre         dense           9351.6254       1.002189e+05\n",
      "4          sabre         sabre           4789.2756       6.969034e+04\n",
      "5          sabre       trivial          10883.4340       7.720734e+04\n",
      "6     stochastic         dense          12644.5284       3.795094e+05\n",
      "7     stochastic         sabre           8840.5944       2.228919e+05\n",
      "8     stochastic       trivial          14199.8710       2.932088e+05\n",
      "\n",
      "Best routing+layout combination per code (across all backends):\n",
      "       code best_routing best_layout  best_value\n",
      "0    bacon        sabre       sabre     892.139\n",
      "1    color        sabre       sabre    5257.670\n",
      "2       hh        sabre       sabre    6201.596\n",
      "3   steane        sabre       sabre    7147.208\n",
      "4  surface        sabre       sabre    4447.765\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------\n",
    "# Load datasets (two files)\n",
    "# -----------------------\n",
    "#df = pd.read_csv(\"../experiment_results/Routing_grid/results.csv\")\n",
    "df = pd.read_csv(\"../experiment_results/Routing_hh/results.csv\")  # change this path as needed\n",
    "\n",
    "# Merge into one dataset\n",
    "#df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# -----------------------\n",
    "# Mean + variance per backend\n",
    "# -----------------------\n",
    "backend_stats = (\n",
    "    df.groupby(\"backend\", as_index=False)\n",
    "      .agg(\n",
    "          gate_overhead_mean=(\"swap_overhead_mean\", \"mean\"),\n",
    "          gate_overhead_var=(\"swap_overhead_var\", \"mean\")  # assumes column exists\n",
    "      )\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Average per code (across all backends)\n",
    "# -----------------------\n",
    "code_avg = df.groupby(\"code\")[\"swap_overhead_mean\"].mean().reset_index()\n",
    "\n",
    "# -----------------------\n",
    "# Average per routing method (across all backends)\n",
    "# -----------------------\n",
    "routing_avg = df.groupby(\"routing_method\")[\"swap_overhead_mean\"].mean().reset_index()\n",
    "\n",
    "# -----------------------\n",
    "# Average per routing + layout method combination\n",
    "# -----------------------\n",
    "combo_avg = (\n",
    "    df.groupby([\"routing_method\", \"layout_method\"], as_index=False)\n",
    "      .agg(\n",
    "          gate_overhead_mean=(\"swap_overhead_mean\", \"mean\"),\n",
    "          gate_overhead_var=(\"swap_overhead_var\", \"mean\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# Pivot table form (just for means)\n",
    "combo_pivot = (\n",
    "    df.pivot_table(\n",
    "        index=\"routing_method\",\n",
    "        columns=\"layout_method\",\n",
    "        values=\"swap_overhead_mean\",\n",
    "        aggfunc=\"mean\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Best routing+layout per code (across all backends)\n",
    "# -----------------------\n",
    "code_combo = (\n",
    "    df.groupby([\"code\", \"routing_method\", \"layout_method\"], as_index=False)\n",
    "      .agg(best_gate_overhead=(\"swap_overhead_mean\", \"mean\"))\n",
    ")\n",
    "\n",
    "best_per_code = (\n",
    "    code_combo.loc[code_combo.groupby(\"code\")[\"best_gate_overhead\"].idxmin()]\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "best_per_code.rename(columns={\n",
    "    \"routing_method\": \"best_routing\",\n",
    "    \"layout_method\": \"best_layout\",\n",
    "    \"best_gate_overhead\": \"best_value\"\n",
    "}, inplace=True)\n",
    "\n",
    "# -----------------------\n",
    "# Print all results\n",
    "# -----------------------\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "general_avg = df[\"swap_overhead_mean\"].mean()\n",
    "print(\"\\nGeneral average across all codes and backends:\", general_avg)\n",
    "sabre_avg = df[\n",
    "    (df[\"routing_method\"] == \"sabre\") & (df[\"layout_method\"] == \"sabre\")\n",
    "][\"swap_overhead_mean\"].mean()\n",
    "\n",
    "print(\"\\nAverage swap_overhead_mean for sabre x sabre combination:\", sabre_avg)\n",
    "\n",
    "sabre_hh_custom = df[\n",
    "    (df[\"code\"] == \"hh\") &\n",
    "    (df[\"backend\"] == \"custom_heavyhex\") &\n",
    "    (df[\"routing_method\"] == \"sabre\") &\n",
    "    (df[\"layout_method\"] == \"sabre\")\n",
    "][\"swap_overhead_mean\"].mean()\n",
    "\n",
    "print(\"\\nAverage swap_overhead_mean for sabre x sabre, code=hh, backend=custom_heavyhex:\", sabre_hh_custom)\n",
    "\n",
    "\n",
    "print(\"Average per backend (mean + var):\\n\", backend_stats)\n",
    "print(\"\\nAverage per code (across all backends):\\n\", code_avg)\n",
    "print(\"\\nAverage per routing method (across all backends):\\n\", routing_avg)\n",
    "print(\"\\nAverage per routing+layout combination (mean + var):\\n\", combo_avg)\n",
    "print(\"\\nBest routing+layout combination per code (across all backends):\\n\", best_per_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40af4048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  translating_method  gate_overhead_mean\n",
      "0             qiskit        27552.000000\n",
      "1   qiskit_optimized        21507.750000\n",
      "2               tket        56552.416667\n",
      "3     tket_optimized        55592.166667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------\n",
    "# Load backend dataset\n",
    "# -----------------------\n",
    "df = pd.read_csv(\"../experiment_results/Translation/results.csv\")\n",
    "\n",
    "# -----------------------\n",
    "# Mean + variance per backend\n",
    "# -----------------------\n",
    "backend_stats = (\n",
    "    df.groupby(\"backend\", as_index=False)\n",
    "      .agg(\n",
    "          gate_overhead_mean=(\"gate_overhead_mean\", \"mean\"),\n",
    "          gate_overhead_var=(\"gate_overhead_var\", \"mean\")  # assumes column exists\n",
    "      )\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Average per code\n",
    "# -----------------------\n",
    "code_avg = df.groupby(\"code\")[\"gate_overhead_mean\"].mean().reset_index()\n",
    "\n",
    "# -----------------------\n",
    "# Average per routing method\n",
    "# -----------------------\n",
    "translating_avg = df.groupby(\"translating_method\")[\"gate_overhead_mean\"].mean().reset_index()\n",
    "\n",
    "print(translating_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f3df617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per code:\n",
      "      code  avg_normalized_overhead\n",
      "0    bacon                 5.301020\n",
      "1    color                 6.504127\n",
      "2    gross                 6.631657\n",
      "3       hh                 6.592105\n",
      "4   steane                 5.509372\n",
      "5  surface                 6.267857\n",
      "\n",
      "Per translating method:\n",
      "  translating_method  avg_normalized_overhead\n",
      "0             qiskit                 4.260593\n",
      "1   qiskit_optimized                 3.166202\n",
      "2               tket                 8.704297\n",
      "3     tket_optimized                 8.406333\n",
      "\n",
      "Per backend:\n",
      "       backend  avg_normalized_overhead\n",
      "0  custom_full                 6.134356\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../experiment_results/Translation/results.csv\")\n",
    "\n",
    "# Compute per-row normalized gate overhead\n",
    "df[\"normalized_overhead\"] = df[\"gate_overhead_mean\"] / df[\"original_total_gates\"]\n",
    "\n",
    "# -----------------------\n",
    "# Average per code\n",
    "# -----------------------\n",
    "code_avg = df.groupby(\"code\")[\"normalized_overhead\"].mean().reset_index()\n",
    "code_avg.rename(columns={\"normalized_overhead\": \"avg_normalized_overhead\"}, inplace=True)\n",
    "\n",
    "# -----------------------\n",
    "# Average per translating method\n",
    "# -----------------------\n",
    "translating_avg = df.groupby(\"translating_method\")[\"normalized_overhead\"].mean().reset_index()\n",
    "translating_avg.rename(columns={\"normalized_overhead\": \"avg_normalized_overhead\"}, inplace=True)\n",
    "\n",
    "# -----------------------\n",
    "# Average per backend\n",
    "# -----------------------\n",
    "backend_avg = df.groupby(\"backend\")[\"normalized_overhead\"].mean().reset_index()\n",
    "backend_avg.rename(columns={\"normalized_overhead\": \"avg_normalized_overhead\"}, inplace=True)\n",
    "\n",
    "print(\"Per code:\")\n",
    "print(code_avg)\n",
    "\n",
    "print(\"\\nPer translating method:\")\n",
    "print(translating_avg)\n",
    "\n",
    "print(\"\\nPer backend:\")\n",
    "print(backend_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa574758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      code  normalized_overhead  proportional_overhead  deviation_from_average\n",
      "0    bacon             5.301020               0.864153               -0.135847\n",
      "1    color             6.504127               1.060279                0.060279\n",
      "2    gross             6.631657               1.081068                0.081068\n",
      "3       hh             6.592105               1.074621                0.074621\n",
      "4   steane             5.509372               0.898117               -0.101883\n",
      "5  surface             6.267857               1.021763                0.021763\n"
     ]
    }
   ],
   "source": [
    "# Compute mean per code (already done)\n",
    "code_avg = df.groupby(\"code\")[\"normalized_overhead\"].mean().reset_index()\n",
    "\n",
    "# Compute global average normalized overhead\n",
    "global_avg = df[\"normalized_overhead\"].mean()\n",
    "\n",
    "# Add proportional and deviation columns\n",
    "code_avg[\"proportional_overhead\"] = code_avg[\"normalized_overhead\"] / global_avg\n",
    "code_avg[\"deviation_from_average\"] = code_avg[\"proportional_overhead\"] - 1\n",
    "\n",
    "print(code_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d7b2959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from qiskit.transpiler import CouplingMap\n",
    "import math\n",
    "d = (2 + math.sqrt(40 * 291 + 24)) / 10\n",
    "d = int(d)\n",
    "if d % 2 == 0:\n",
    "    d -= 1\n",
    "print(d)\n",
    "coupling_map = CouplingMap.from_heavy_hex(d)\n",
    "print(coupling_map.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56817ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        backend  backend_size     code      decoder  distance  cycles  \\\n",
      "1   custom_full           400   steane         mwpm         9       9   \n",
      "2   custom_full           400  surface         mwpm        13      13   \n",
      "3   custom_full           400       hh         mwpm        11      11   \n",
      "4   custom_full           400    color         mwpm        13      13   \n",
      "9   custom_full           400    bacon         mwpm        19      19   \n",
      "10  custom_full           400    gross    bposd_chk        12      12   \n",
      "11  custom_full           400    gross  bposd_batch        12      12   \n",
      "14  custom_full           400   steane    bposd_chk         9       9   \n",
      "15  custom_full           400   steane  bposd_batch         9       9   \n",
      "20  custom_full           400       hh    bposd_chk        11      11   \n",
      "21  custom_full           400       hh  bposd_batch        11      11   \n",
      "22  custom_full           400  surface    bposd_chk        13      13   \n",
      "24  custom_full           400  surface  bposd_batch        13      13   \n",
      "26  custom_full           400    bacon  bposd_batch        19      19   \n",
      "27  custom_full           400    bacon    bposd_chk        19      19   \n",
      "30  custom_full           400    color  bposd_batch        13      13   \n",
      "31  custom_full           400    color    bposd_chk        13      13   \n",
      "\n",
      "    num_samples error_type  error_probability  logical_error_rate  \\\n",
      "1          1000   constant              0.004               0.286   \n",
      "2          1000   constant              0.004               0.029   \n",
      "3          1000   constant              0.004               0.482   \n",
      "4          1000   constant              0.004               0.487   \n",
      "9          1000   constant              0.004               0.515   \n",
      "10         1000   constant              0.004               0.283   \n",
      "11         1000   constant              0.004               0.302   \n",
      "14         1000   constant              0.004               0.326   \n",
      "15         1000   constant              0.004               0.309   \n",
      "20         1000   constant              0.004               0.477   \n",
      "21         1000   constant              0.004               0.485   \n",
      "22         1000   constant              0.004               0.022   \n",
      "24         1000   constant              0.004               0.033   \n",
      "26         1000   constant              0.004               0.502   \n",
      "27         1000   constant              0.004               0.494   \n",
      "30         1000   constant              0.004               0.243   \n",
      "31         1000   constant              0.004               0.249   \n",
      "\n",
      "    layout_method  routing_method  translating_method     exec_time  \n",
      "1             NaN             NaN                 NaN      0.175250  \n",
      "2             NaN             NaN                 NaN      0.310198  \n",
      "3             NaN             NaN                 NaN      0.330240  \n",
      "4             NaN             NaN                 NaN      0.487455  \n",
      "9             NaN             NaN                 NaN      0.611859  \n",
      "10            NaN             NaN                 NaN   4592.632116  \n",
      "11            NaN             NaN                 NaN   4627.180782  \n",
      "14            NaN             NaN                 NaN   7209.122726  \n",
      "15            NaN             NaN                 NaN   7217.867864  \n",
      "20            NaN             NaN                 NaN  20436.631934  \n",
      "21            NaN             NaN                 NaN  20506.134269  \n",
      "22            NaN             NaN                 NaN  34813.058622  \n",
      "24            NaN             NaN                 NaN  35090.129063  \n",
      "26            NaN             NaN                 NaN  39125.388035  \n",
      "27            NaN             NaN                 NaN  39613.959832  \n",
      "30            NaN             NaN                 NaN  59967.093388  \n",
      "31            NaN             NaN                 NaN  61220.143108  \n",
      "=== General Decoders Average (modsi1000 only, excluding 'gross') ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decoder</th>\n",
       "      <th>logical_error_rate</th>\n",
       "      <th>exec_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bposd_batch</td>\n",
       "      <td>0.312333</td>\n",
       "      <td>27755.632234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>27980.924723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mwpm</td>\n",
       "      <td>0.359800</td>\n",
       "      <td>0.383000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       decoder  logical_error_rate     exec_time\n",
       "0  bposd_batch            0.312333  27755.632234\n",
       "1    bposd_chk            0.308500  27980.924723\n",
       "2         mwpm            0.359800      0.383000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Best Decoder per Code (lowest logical error rate) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64019/3767896263.py:38: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.loc[x[\"logical_error_rate\"].idxmin()])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>decoder</th>\n",
       "      <th>logical_error_rate</th>\n",
       "      <th>exec_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacon</td>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.494</td>\n",
       "      <td>39613.959832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Color</td>\n",
       "      <td>bposd_batch</td>\n",
       "      <td>0.243</td>\n",
       "      <td>59967.093388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gross</td>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.283</td>\n",
       "      <td>4592.632116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hh</td>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.477</td>\n",
       "      <td>20436.631934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steane</td>\n",
       "      <td>mwpm</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.175250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Surface</td>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.022</td>\n",
       "      <td>34813.058622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      code      decoder  logical_error_rate     exec_time\n",
       "0    Bacon    bposd_chk               0.494  39613.959832\n",
       "1    Color  bposd_batch               0.243  59967.093388\n",
       "2    Gross    bposd_chk               0.283   4592.632116\n",
       "3       Hh    bposd_chk               0.477  20436.631934\n",
       "4   Steane         mwpm               0.286      0.175250\n",
       "5  Surface    bposd_chk               0.022  34813.058622"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === General decoders summary with best decoder per code ===\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df_general = pd.read_csv(\"../experiment_results/Decoders/results_general.csv\")\n",
    "\n",
    "\n",
    "# Filter only modsi1000 results and exclude gross code\n",
    "df_general = df_general[(df_general[\"error_type\"] == \"constant\")] #& (df_general[\"code\"].str.lower() != \"gross\")]\n",
    "print(df_general)\n",
    "\n",
    "# Optional renaming\n",
    "if \"decoder_map\" in globals():\n",
    "    df_general[\"decoder\"] = df_general[\"decoder\"].apply(lambda x: decoder_map.get(x, x))\n",
    "if \"code_rename_map\" in globals():\n",
    "    df_general[\"code\"] = df_general[\"code\"].apply(lambda x: code_rename_map.get(x.lower(), x.capitalize()))\n",
    "else:\n",
    "    df_general[\"code\"] = df_general[\"code\"].apply(lambda x: x.capitalize())\n",
    "\n",
    "# Compute mean per decoder\n",
    "summary_general = (\n",
    "    df_general\n",
    "    .groupby(\"decoder\", as_index=False)\n",
    "    .agg({\n",
    "        \"logical_error_rate\": \"mean\",\n",
    "        \"exec_time\": \"mean\"\n",
    "    })\n",
    "    .sort_values(\"decoder\")\n",
    ")\n",
    "\n",
    "print(\"=== General Decoders Average (modsi1000 only, excluding 'gross') ===\")\n",
    "display(summary_general)\n",
    "\n",
    "# Determine best decoder per code\n",
    "best_per_code = (\n",
    "    df_general\n",
    "    .groupby(\"code\")\n",
    "    .apply(lambda x: x.loc[x[\"logical_error_rate\"].idxmin()])\n",
    "    .reset_index(drop=True)\n",
    "    [[\"code\", \"decoder\", \"logical_error_rate\", \"exec_time\"]]\n",
    ")\n",
    "\n",
    "print(\"=== Best Decoder per Code (lowest logical error rate) ===\")\n",
    "display(best_per_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aba351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gap between best and worst decoder per code ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64019/1188751606.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gap_df = df_general.groupby(\"code\").apply(best_worst_gap).reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>best_decoder</th>\n",
       "      <th>best_rate</th>\n",
       "      <th>worst_decoder</th>\n",
       "      <th>worst_rate</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacon</td>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.494</td>\n",
       "      <td>mwpm</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Color</td>\n",
       "      <td>bposd_batch</td>\n",
       "      <td>0.243</td>\n",
       "      <td>mwpm</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gross</td>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.283</td>\n",
       "      <td>bposd_batch</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hh</td>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.477</td>\n",
       "      <td>bposd_batch</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steane</td>\n",
       "      <td>mwpm</td>\n",
       "      <td>0.286</td>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Surface</td>\n",
       "      <td>bposd_chk</td>\n",
       "      <td>0.022</td>\n",
       "      <td>bposd_batch</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      code best_decoder  best_rate worst_decoder  worst_rate    gap\n",
       "0    Bacon    bposd_chk      0.494          mwpm       0.515  0.021\n",
       "1    Color  bposd_batch      0.243          mwpm       0.487  0.244\n",
       "2    Gross    bposd_chk      0.283   bposd_batch       0.302  0.019\n",
       "3       Hh    bposd_chk      0.477   bposd_batch       0.485  0.008\n",
       "4   Steane         mwpm      0.286     bposd_chk       0.326  0.040\n",
       "5  Surface    bposd_chk      0.022   bposd_batch       0.033  0.011"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Code with the largest difference between best and worst decoder:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "code                   Color\n",
       "best_decoder     bposd_batch\n",
       "best_rate              0.243\n",
       "worst_decoder           mwpm\n",
       "worst_rate             0.487\n",
       "gap                    0.244\n",
       "Name: 1, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_general is already filtered for modsi1000 and excludes \"gross\"\n",
    "\n",
    "def best_worst_gap(group):\n",
    "    # Sort decoders by logical_error_rate\n",
    "    sorted_rates = group.sort_values(\"logical_error_rate\")\n",
    "    best_rate = sorted_rates.iloc[0][\"logical_error_rate\"]\n",
    "    worst_rate = sorted_rates.iloc[-1][\"logical_error_rate\"]\n",
    "    gap = worst_rate - best_rate\n",
    "    best_decoder = sorted_rates.iloc[0][\"decoder\"]\n",
    "    worst_decoder = sorted_rates.iloc[-1][\"decoder\"]\n",
    "    \n",
    "    return pd.Series({\n",
    "        \"best_decoder\": best_decoder,\n",
    "        \"best_rate\": best_rate,\n",
    "        \"worst_decoder\": worst_decoder,\n",
    "        \"worst_rate\": worst_rate,\n",
    "        \"gap\": gap\n",
    "    })\n",
    "\n",
    "# Apply per code\n",
    "gap_df = df_general.groupby(\"code\").apply(best_worst_gap).reset_index()\n",
    "\n",
    "# Find code with the largest gap\n",
    "max_gap_row = gap_df.loc[gap_df[\"gap\"].idxmax()]\n",
    "\n",
    "print(\"=== Gap between best and worst decoder per code ===\")\n",
    "display(gap_df)\n",
    "\n",
    "print(\"\\nCode with the largest difference between best and worst decoder:\")\n",
    "display(max_gap_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b587d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Special Decoders Summary (modsi1000 only) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decoder</th>\n",
       "      <th>code</th>\n",
       "      <th>logical_error_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bposd</td>\n",
       "      <td>Color</td>\n",
       "      <td>0.003633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chromobius</td>\n",
       "      <td>Color</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      decoder   code  logical_error_rate\n",
       "0       bposd  Color            0.003633\n",
       "1  chromobius  Color            0.001100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Special decoders summary ===\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "df_special = pd.read_csv(\"../experiment_results/Decoders/results_special.csv\")\n",
    "\n",
    "# Filter only modsi1000 results\n",
    "df_special = df_special[df_special[\"error_type\"] == \"phenomenological\"]\n",
    "\n",
    "# Optional renaming (if maps defined in globals)\n",
    "if \"decoder_map\" in globals():\n",
    "    df_special[\"decoder\"] = df_special[\"decoder\"].apply(lambda x: decoder_map.get(x, x))\n",
    "if \"code_rename_map\" in globals():\n",
    "    df_special[\"code\"] = df_special[\"code\"].apply(lambda x: code_rename_map.get(x.lower(), x.capitalize()))\n",
    "else:\n",
    "    df_special[\"code\"] = df_special[\"code\"].apply(lambda x: x.capitalize())\n",
    "\n",
    "# Compute mean logical error rate and execution time per decoder and per code\n",
    "summary_special = (\n",
    "    df_special\n",
    "    .groupby([\"decoder\", \"code\"], as_index=False)\n",
    "    .agg({\n",
    "        \"logical_error_rate\": \"mean\"\n",
    "        \n",
    "    })\n",
    "    .sort_values([\"decoder\", \"code\"])\n",
    ")\n",
    "\n",
    "# Display nicely\n",
    "print(\"=== Special Decoders Summary (modsi1000 only) ===\")\n",
    "display(summary_special)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
